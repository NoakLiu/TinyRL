#!/usr/bin/env python3
"""
æœ€å°æ¼”ç¤ºï¼šä½¿ç”¨GPT2å’ŒTinyRLç»„ä»¶

æ¼”ç¤º1ï¼šä¸€ä¸ªLLMåœ¨ä¸€ä¸ªæ²™ç›’é‡Œè¿è¡Œ
æ¼”ç¤º2ï¼šä¸€ä¸ªLLMåŒæ—¶åœ¨ä¸¤ä¸ªæ²™ç›’é‡Œäº¤äº’
æ¼”ç¤º3ï¼šä¸¤ä¸ªLLMåœ¨ä¸€ä¸ªæ²™ç›’ä¸­åä½œ
åŒ…æ‹¬ï¼šMCPåè®®å®šä¹‰ã€å¼€å‘æ ‡å‡†ã€è¯„ä»·æŒ‡æ ‡ã€å¤šä»£ç†å¼ºåŒ–å­¦ä¹ 
"""

import asyncio
import sys
import os
import time
import json
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
from enum import Enum

# Add current directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# ==================== MCP (Model Context Protocol) å®šä¹‰ ====================

class MCPTaskType(Enum):
    """MCPä»»åŠ¡ç±»å‹"""
    CODE_GENERATION = "code_generation"
    DATA_ANALYSIS = "data_analysis"
    PLANNING = "planning"
    EXECUTION = "execution"
    REVIEW = "review"
    TESTING = "testing"
    OPTIMIZATION = "optimization"
    REINFORCEMENT_LEARNING = "reinforcement_learning"

@dataclass
class MCPMessage:
    """MCPæ¶ˆæ¯åè®®"""
    id: str
    sender: str
    receiver: str
    task_type: MCPTaskType
    content: str
    context: Dict[str, Any]
    timestamp: float
    priority: int = 1  # 1-5, 5æœ€é«˜

@dataclass
class MCPTask:
    """MCPä»»åŠ¡å®šä¹‰"""
    id: str
    title: str
    description: str
    task_type: MCPTaskType
    requirements: List[str]
    success_criteria: List[str]
    estimated_time: int  # ç§’
    dependencies: List[str] = None

    def __post_init__(self):
        if self.dependencies is None:
            self.dependencies = []

@dataclass
class MCPEvaluationMetrics:
    """MCPè¯„ä»·æŒ‡æ ‡"""
    task_success_rate: float = 0.0
    execution_time: float = 0.0
    code_quality_score: float = 0.0
    collaboration_efficiency: float = 0.0
    resource_usage: float = 0.0
    error_rate: float = 0.0
    innovation_score: float = 0.0

class MCPDevelopmentStandards:
    """MCPå¼€å‘æ ‡å‡†"""
    
    @staticmethod
    def get_code_quality_standards() -> Dict[str, Any]:
        """ä»£ç è´¨é‡æ ‡å‡†"""
        return {
            "naming_convention": "snake_case",
            "max_function_length": 50,
            "max_line_length": 88,
            "docstring_required": True,
            "type_hints_required": True,
            "error_handling_required": True,
            "test_coverage_min": 0.8
        }
    
    @staticmethod
    def get_collaboration_protocols() -> Dict[str, Any]:
        """åä½œåè®®"""
        return {
            "message_format": "MCPMessage",
            "task_handoff_required": True,
            "progress_reporting_interval": 10,  # seconds
            "conflict_resolution": "majority_vote",
            "timeout_handling": "graceful_degradation"
        }
    
    @staticmethod
    def get_rl_game_standards() -> Dict[str, Any]:
        """å¼ºåŒ–å­¦ä¹ æ¸¸æˆæ ‡å‡†"""
        return {
            "environment_interface": "gym",
            "action_space": "discrete_or_continuous",
            "observation_space": "defined",
            "reward_function": "sparse_or_dense",
            "episode_termination": "defined",
            "evaluation_episodes": 100,
            "max_steps_per_episode": 1000
        }

class MCPAgent:
    """MCPä»£ç†åŸºç±»"""
    
    def __init__(self, agent_id: str, role: str, llm_model):
        self.agent_id = agent_id
        self.role = role
        self.llm_model = llm_model
        self.message_history: List[MCPMessage] = []
        self.tasks_completed: List[MCPTask] = []
        self.metrics = MCPEvaluationMetrics()
    
    async def process_message(self, message: MCPMessage) -> MCPMessage:
        """å¤„ç†MCPæ¶ˆæ¯"""
        self.message_history.append(message)
        
        # æ ¹æ®ä»»åŠ¡ç±»å‹ç”Ÿæˆå“åº”
        response_content = await self._generate_response(message)
        
        response = MCPMessage(
            id=f"{self.agent_id}_{len(self.message_history)}",
            sender=self.agent_id,
            receiver=message.sender,
            task_type=message.task_type,
            content=response_content,
            context={"response_to": message.id},
            timestamp=time.time()
        )
        
        return response
    
    async def _generate_response(self, message: MCPMessage) -> str:
        """ç”Ÿæˆå“åº”å†…å®¹"""
        prompt = f"""
Role: {self.role}
Task Type: {message.task_type.value}
Request: {message.content}
Context: {json.dumps(message.context)}

Please provide a detailed response for this {message.task_type.value} task.
"""
        return self.llm_model.generate(prompt)

class MCPEvaluator:
    """MCPè¯„ä»·å™¨"""
    
    @staticmethod
    def evaluate_task_success(task: MCPTask, result: str) -> float:
        """è¯„ä¼°ä»»åŠ¡æˆåŠŸç‡"""
        success_indicators = 0
        total_criteria = len(task.success_criteria)
        
        for criteria in task.success_criteria:
            if criteria.lower() in result.lower():
                success_indicators += 1
        
        return success_indicators / total_criteria if total_criteria > 0 else 0.0
    
    @staticmethod
    def evaluate_code_quality(code: str) -> float:
        """è¯„ä¼°ä»£ç è´¨é‡"""
        quality_score = 0.0
        max_score = 100.0
        
        # åŸºæœ¬æ£€æŸ¥
        if "def " in code:
            quality_score += 20  # æœ‰å‡½æ•°å®šä¹‰
        if "import " in code:
            quality_score += 10  # æœ‰æ¨¡å—å¯¼å…¥
        if "#" in code:
            quality_score += 15  # æœ‰æ³¨é‡Š
        if "try:" in code and "except:" in code:
            quality_score += 20  # æœ‰é”™è¯¯å¤„ç†
        if len(code.split('\n')) > 5:
            quality_score += 15  # ä»£ç é•¿åº¦åˆç†
        if not any(line.strip() for line in code.split('\n') if len(line.strip()) > 100):
            quality_score += 20  # è¡Œé•¿åº¦åˆç†
        
        return min(quality_score / max_score, 1.0)
    
    @staticmethod
    def evaluate_collaboration_efficiency(agents: List[MCPAgent], total_time: float) -> float:
        """è¯„ä¼°åä½œæ•ˆç‡"""
        total_messages = sum(len(agent.message_history) for agent in agents)
        if total_messages == 0 or total_time == 0:
            return 0.0
        
        # æ¶ˆæ¯äº¤æ¢é¢‘ç‡å’Œæ—¶é—´æ•ˆç‡
        message_rate = total_messages / total_time
        return min(message_rate / 10.0, 1.0)  # å½’ä¸€åŒ–åˆ°0-1

# ==================== ç®€åŒ–çš„GPT2å®¢æˆ·ç«¯ ====================

class SimpleGPT2:
    """ç®€åŒ–çš„GPT2å®¢æˆ·ç«¯"""
    
    def __init__(self, model_id: str = "gpt2"):
        self.model_id = model_id
        self.model = None
        self.use_mock = False
        self._load_model()
    
    def _load_model(self):
        """å»¶è¿ŸåŠ è½½æ¨¡å‹"""
        try:
            from transformers import pipeline
            print(f"ğŸ“¦ Loading {self.model_id} model...")
            self.model = pipeline(
                "text-generation",
                model="gpt2",
                device="cpu",
                max_length=200
            )
            print(f"âœ… {self.model_id} model loaded successfully!")
        except ImportError:
            print(f"âŒ transformers not installed. Using mock mode for {self.model_id}")
            self.use_mock = True
            self.model = None
        except Exception as e:
            print(f"âŒ Failed to load {self.model_id}: {e}. Using mock mode.")
            self.use_mock = True
            self.model = None
    
    def generate(self, prompt: str) -> str:
        """ç”Ÿæˆæ–‡æœ¬"""
        if self.use_mock or not self.model:
            # Mock response for demo purposes
            if "strategy" in prompt.lower() or "planning" in prompt.lower():
                return """
I recommend using a binary search strategy for the number guessing game:
1. Start with the middle value (50)
2. Based on feedback, adjust the search range
3. Continue halving the search space
4. This approach guarantees finding the target in log(n) steps
5. Implementation should track low/high bounds
"""
            elif "execute" in prompt.lower() or "optimize" in prompt.lower():
                return """
Execution plan for RL strategy:
1. Load the binary search agent code
2. Run multiple episodes with different targets
3. Collect performance metrics (steps taken, success rate)
4. Analyze efficiency compared to random search
5. Generate comprehensive performance report
"""
            else:
                return f"""
Mock GPT-2 response for: {prompt[:50]}...
This is a simulated response since transformers is not available.
The response would normally be generated by the GPT-2 model.
"""
        
        # Real model execution
        # ä¸ºä»£ç ç”Ÿæˆä¼˜åŒ–æç¤ºè¯
        if "def " in prompt or "import " in prompt or "class " in prompt:
            enhanced_prompt = f"# Python code\n{prompt}"
        else:
            enhanced_prompt = prompt
        
        try:
            outputs = self.model(
                enhanced_prompt,
                max_new_tokens=100,
                temperature=0.7,
                do_sample=True,
                pad_token_id=self.model.tokenizer.eos_token_id,
                num_return_sequences=1
            )
            
            result = outputs[0]["generated_text"]
            # åªè¿”å›æ–°ç”Ÿæˆçš„éƒ¨åˆ†
            if result.startswith(enhanced_prompt):
                result = result[len(enhanced_prompt):].strip()
            
            return result
        except Exception as e:
            print(f"Model generation failed: {e}")
            return f"Error generating response: {str(e)}"

# ==================== å¼ºåŒ–å­¦ä¹ æ¸¸æˆç¯å¢ƒ ====================

class SimpleRLGame:
    """ç®€å•çš„å¼ºåŒ–å­¦ä¹ æ¸¸æˆç¯å¢ƒ"""
    
    def __init__(self, game_type: str = "number_guessing"):
        self.game_type = game_type
        self.state: Optional[Dict[str, Any]] = None
        self.target: Optional[int] = None
        self.steps = 0
        self.max_steps = 10
        self.reset()
    
    def reset(self):
        """é‡ç½®æ¸¸æˆ"""
        import random
        self.target = random.randint(1, 100)
        self.state = {"current_guess": None, "feedback": "start", "steps_remaining": self.max_steps}
        self.steps = 0
        return self.state
    
    def step(self, action: int):
        """æ‰§è¡ŒåŠ¨ä½œ"""
        if self.state is None or self.target is None:
            raise ValueError("Game not initialized. Call reset() first.")
            
        self.steps += 1
        self.state["current_guess"] = action
        self.state["steps_remaining"] = self.max_steps - self.steps
        
        # è®¡ç®—å¥–åŠ±
        if action == self.target:
            reward = 100 - self.steps  # è¶Šå¿«çŒœä¸­å¥–åŠ±è¶Šé«˜
            done = True
            self.state["feedback"] = f"Correct! Target was {self.target}"
        elif action < self.target:
            reward = -abs(action - self.target) / 100
            done = False
            self.state["feedback"] = "Too low"
        else:
            reward = -abs(action - self.target) / 100
            done = False
            self.state["feedback"] = "Too high"
        
        if self.steps >= self.max_steps:
            done = True
            if action != self.target:
                self.state["feedback"] = f"Game over! Target was {self.target}"
        
        return self.state, reward, done
    
    def get_code_template(self) -> str:
        """è·å–æ¸¸æˆä»£ç æ¨¡æ¿"""
        return """
# Simple RL Agent for Number Guessing Game
import random

class SimpleAgent:
    def __init__(self):
        self.low = 1
        self.high = 100
        self.history = []
    
    def choose_action(self, state):
        # Binary search strategy
        if state['feedback'] == 'start':
            guess = (self.low + self.high) // 2
        elif state['feedback'] == 'Too low':
            self.low = state['current_guess'] + 1
            guess = (self.low + self.high) // 2
        elif state['feedback'] == 'Too high':
            self.high = state['current_guess'] - 1
            guess = (self.low + self.high) // 2
        else:
            guess = random.randint(self.low, self.high)
        
        self.history.append(guess)
        return guess
    
    def get_performance_metrics(self):
        return {
            'total_guesses': len(self.history),
            'efficiency': 1.0 / len(self.history) if self.history else 0,
            'strategy': 'binary_search'
        }
"""

# ==================== Mock Sandbox Classes (fallback when agents.sandbox not available) ====================

class MockSandbox:
    """Mockæ²™ç›’ç±»ï¼Œç”¨äºæ¼”ç¤º"""
    def __init__(self, sandbox_id: str):
        self.sandbox_id = sandbox_id
        self.files: Dict[str, str] = {}
    
    async def initialize(self):
        """åˆå§‹åŒ–æ²™ç›’"""
        pass
    
    def execute_code(self, code: str, filename: str):
        """æ‰§è¡Œä»£ç """
        from types import SimpleNamespace
        try:
            # ç®€å•çš„ä»£ç æ‰§è¡Œæ¨¡æ‹Ÿ
            if "print(" in code:
                import re
                prints = re.findall(r'print\((.*?)\)', code)
                output = "\n".join([f"Mock output: {p}" for p in prints[:5]])
            else:
                output = "Mock code execution successful"
            
            return SimpleNamespace(
                success=True,
                output=output,
                error=None,
                execution_time=0.1
            )
        except Exception as e:
            return SimpleNamespace(
                success=False,
                output="",
                error=str(e),
                execution_time=0.1
            )
    
    def write_file(self, filename: str, content: str):
        """å†™å…¥æ–‡ä»¶"""
        self.files[filename] = content
    
    def read_file(self, filename: str) -> str:
        """è¯»å–æ–‡ä»¶"""
        return self.files.get(filename, f"Mock content of {filename}")
    
    def list_files(self) -> List[str]:
        """åˆ—å‡ºæ–‡ä»¶"""
        return list(self.files.keys())

class MockSandboxManager:
    """Mockæ²™ç›’ç®¡ç†å™¨"""
    def __init__(self):
        self.sandboxes: Dict[str, MockSandbox] = {}
    
    async def create_sandbox(self, config) -> str:
        """åˆ›å»ºæ²™ç›’"""
        sandbox_id = f"mock_{config.name}_{len(self.sandboxes)}"
        self.sandboxes[sandbox_id] = MockSandbox(sandbox_id)
        return sandbox_id
    
    def get_sandbox(self, sandbox_id: str) -> Optional[MockSandbox]:
        """è·å–æ²™ç›’"""
        return self.sandboxes.get(sandbox_id)
    
    async def destroy_sandbox(self, sandbox_id: str):
        """é”€æ¯æ²™ç›’"""
        if sandbox_id in self.sandboxes:
            del self.sandboxes[sandbox_id]

class MockSandboxConfig:
    """Mockæ²™ç›’é…ç½®"""
    def __init__(self, name: str, timeout: int = 30, max_memory_mb: int = 512):
        self.name = name
        self.timeout = timeout
        self.max_memory_mb = max_memory_mb

# ==================== æ¼”ç¤ºå‡½æ•° ====================

async def demo1_one_llm_one_sandbox():
    """æ¼”ç¤º1ï¼šä¸€ä¸ªLLMåœ¨ä¸€ä¸ªæ²™ç›’é‡Œè¿è¡Œ"""
    print("\n" + "="*60)
    print("ğŸš€ æ¼”ç¤º1ï¼šä¸€ä¸ªLLMåœ¨ä¸€ä¸ªæ²™ç›’é‡Œè¿è¡Œ")
    print("="*60)
    
    try:
        # å°è¯•å¯¼å…¥çœŸå®æ²™ç›’ç»„ä»¶ï¼Œå¤±è´¥åˆ™ä½¿ç”¨Mock
        try:
            from agents.sandbox import SandboxManager, SandboxConfig
        except ImportError:
            print("ğŸ“¦ Using mock sandbox for demo...")
            SandboxManager = MockSandboxManager  # type: ignore
            SandboxConfig = MockSandboxConfig    # type: ignore
        
        # åˆå§‹åŒ–LLM
        llm = SimpleGPT2()
        
        # åˆ›å»ºæ²™ç›’ç®¡ç†å™¨
        manager = SandboxManager()
        
        # åˆ›å»ºæ²™ç›’é…ç½®
        config = SandboxConfig(
            name="demo_sandbox_1",
            timeout=30,
            max_memory_mb=512
        )
        
        print("ğŸ”§ Creating sandbox...")
        sandbox_id = await manager.create_sandbox(config)
        sandbox = manager.get_sandbox(sandbox_id)
        
        if sandbox is None:
            print("âŒ Failed to create sandbox")
            return False
        
        # åˆå§‹åŒ–æ²™ç›’
        await sandbox.initialize()
        print(f"âœ… Sandbox created: {sandbox_id}")
        
        # è®©LLMç”Ÿæˆä»£ç 
        prompt = "def calculate_fibonacci(n):\n    # Calculate fibonacci sequence"
        print(f"\nğŸ“ Prompting LLM: {prompt}")
        
        generated_code = llm.generate(prompt)
        print(f"ğŸ¤– LLM Generated:\n{generated_code}")
        
        # å®Œæ•´çš„ä»£ç 
        full_code = f"""def calculate_fibonacci(n):
    if n <= 1:
        return n
    else:
        return calculate_fibonacci(n-1) + calculate_fibonacci(n-2)

# Test the function
for i in range(10):
    print(f"fibonacci({{i}}) = {{calculate_fibonacci(i)}}")
"""
        
        print(f"\nğŸ”§ Executing code in sandbox...")
        result = sandbox.execute_code(full_code, "fibonacci.py")
        
        if result.success:
            print(f"âœ… Execution successful!")
            print(f"ğŸ“Š Output:\n{result.output}")
            print(f"â±ï¸ Execution time: {result.execution_time:.2f}s")
        else:
            print(f"âŒ Execution failed:")
            print(f"Error: {result.error}")
        
        # æ¸…ç†
        await manager.destroy_sandbox(sandbox_id)
        print("ğŸ§¹ Sandbox cleaned up")
        
        return result.success
        
    except Exception as e:
        print(f"âŒ Demo 1 failed: {e}")
        return False

async def demo2_one_llm_two_sandboxes():
    """æ¼”ç¤º2ï¼šä¸€ä¸ªLLMåŒæ—¶åœ¨ä¸¤ä¸ªæ²™ç›’é‡Œäº¤äº’"""
    print("\n" + "="*60)
    print("ğŸš€ æ¼”ç¤º2ï¼šä¸€ä¸ªLLMåŒæ—¶åœ¨ä¸¤ä¸ªæ²™ç›’é‡Œäº¤äº’")
    print("="*60)
    
    try:
        # å°è¯•å¯¼å…¥çœŸå®æ²™ç›’ç»„ä»¶ï¼Œå¤±è´¥åˆ™ä½¿ç”¨Mock
        try:
            from agents.sandbox import SandboxManager, SandboxConfig
        except ImportError:
            print("ğŸ“¦ Using mock sandbox for demo...")
            SandboxManager = MockSandboxManager  # type: ignore
            SandboxConfig = MockSandboxConfig    # type: ignore
        
        # åˆå§‹åŒ–LLM
        llm = SimpleGPT2()
        
        # åˆ›å»ºæ²™ç›’ç®¡ç†å™¨
        manager = SandboxManager()
        
        # åˆ›å»ºä¸¤ä¸ªæ²™ç›’é…ç½®
        config1 = SandboxConfig(
            name="data_generator",
            timeout=30,
            max_memory_mb=512
        )
        
        config2 = SandboxConfig(
            name="data_processor", 
            timeout=30,
            max_memory_mb=512
        )
        
        print("ğŸ”§ Creating two sandboxes...")
        sandbox1_id = await manager.create_sandbox(config1)
        sandbox2_id = await manager.create_sandbox(config2)
        
        sandbox1 = manager.get_sandbox(sandbox1_id)
        sandbox2 = manager.get_sandbox(sandbox2_id)
        
        if sandbox1 is None or sandbox2 is None:
            print("âŒ Failed to create sandboxes")
            return False
        
        # åˆå§‹åŒ–æ²™ç›’
        await sandbox1.initialize()
        await sandbox2.initialize()
        print(f"âœ… Sandbox 1 (Data Generator): {sandbox1_id}")
        print(f"âœ… Sandbox 2 (Data Processor): {sandbox2_id}")
        
        # ç¬¬ä¸€æ­¥ï¼šåœ¨æ²™ç›’1ä¸­ç”Ÿæˆæ•°æ®
        print(f"\nğŸ“ Step 1: Generate data in Sandbox 1")
        
        data_gen_prompt = "import json\ndata = []"
        print(f"Prompting LLM for data generation...")
        
        # æ•°æ®ç”Ÿæˆä»£ç 
        data_gen_code = """import json
import random

# Generate sample sales data
data = []
for i in range(20):
    sale = {
        'id': i + 1,
        'product': f'Product_{chr(65 + i % 5)}',
        'price': round(random.uniform(10, 100), 2),
        'quantity': random.randint(1, 10),
        'date': f'2024-{random.randint(1, 12):02d}-{random.randint(1, 28):02d}'
    }
    data.append(sale)

# Save to file
with open('sales_data.json', 'w') as f:
    json.dump(data, f, indent=2)

print(f"Generated {len(data)} sales records")
print("Sample data:", data[:3])
"""
        
        print("ğŸ”§ Executing data generation in Sandbox 1...")
        result1 = sandbox1.execute_code(data_gen_code, "generate_data.py")
        
        if result1.success:
            print("âœ… Data generation successful!")
            print(f"ğŸ“Š Output:\n{result1.output}")
        else:
            print(f"âŒ Data generation failed: {result1.error}")
            return False
        
        # ç¬¬äºŒæ­¥ï¼šè¯»å–æ•°æ®æ–‡ä»¶
        try:
            data_content = sandbox1.read_file("sales_data.json")
            print("ğŸ“„ Data file read successfully")
        except Exception as e:
            print(f"âŒ Failed to read data file: {e}")
            return False
        
        # ç¬¬ä¸‰æ­¥ï¼šåœ¨æ²™ç›’2ä¸­å¤„ç†æ•°æ®
        print(f"\nğŸ“ Step 2: Process data in Sandbox 2")
        
        # åœ¨æ²™ç›’2ä¸­å†™å…¥æ•°æ®æ–‡ä»¶
        sandbox2.write_file("sales_data.json", data_content)
        
        # æ•°æ®å¤„ç†ä»£ç 
        data_process_code = """import json

# Load data
with open('sales_data.json', 'r') as f:
    data = json.load(f)

# Process data - calculate statistics
total_sales = sum(item['price'] * item['quantity'] for item in data)
product_counts = {}
for item in data:
    product = item['product']
    product_counts[product] = product_counts.get(product, 0) + item['quantity']

# Generate report
report = {
    'total_records': len(data),
    'total_sales_value': round(total_sales, 2),
    'product_summary': product_counts,
    'average_sale_value': round(total_sales / len(data), 2)
}

# Save report
with open('sales_report.json', 'w') as f:
    json.dump(report, f, indent=2)

print("ğŸ“Š Sales Analysis Report:")
print(f"Total Records: {report['total_records']}")
print(f"Total Sales Value: ${report['total_sales_value']}")
print(f"Average Sale Value: ${report['average_sale_value']}")
print("Product Summary:", report['product_summary'])
"""
        
        print("ğŸ”§ Executing data processing in Sandbox 2...")
        result2 = sandbox2.execute_code(data_process_code, "process_data.py")
        
        if result2.success:
            print("âœ… Data processing successful!")
            print(f"ğŸ“Š Output:\n{result2.output}")
        else:
            print(f"âŒ Data processing failed: {result2.error}")
            return False
        
        # ç¬¬å››æ­¥ï¼šå±•ç¤ºè·¨æ²™ç›’åä½œç»“æœ
        print(f"\nğŸ“ˆ Cross-Sandbox Collaboration Results:")
        print(f"ğŸ“ Files in Sandbox 1: {sandbox1.list_files()}")
        print(f"ğŸ“ Files in Sandbox 2: {sandbox2.list_files()}")
        
        # è¯»å–æœ€ç»ˆæŠ¥å‘Š
        try:
            report_content = sandbox2.read_file("sales_report.json")
            print(f"ğŸ“‹ Final Report Generated:\n{report_content}")
        except Exception as e:
            print(f"âŒ Failed to read report: {e}")
        
        # æ¸…ç†
        await manager.destroy_sandbox(sandbox1_id)
        await manager.destroy_sandbox(sandbox2_id)
        print("ğŸ§¹ Both sandboxes cleaned up")
        
        return True
        
    except Exception as e:
        print(f"âŒ Demo 2 failed: {e}")
        return False

async def demo3_two_llms_one_sandbox():
    """æ¼”ç¤º3ï¼šä¸¤ä¸ªLLMåœ¨ä¸€ä¸ªæ²™ç›’ä¸­åä½œ - å¤šä»£ç†å¼ºåŒ–å­¦ä¹ """
    print("\n" + "="*60)
    print("ğŸš€ æ¼”ç¤º3ï¼šä¸¤ä¸ªLLMåœ¨ä¸€ä¸ªæ²™ç›’ä¸­åä½œ - å¤šä»£ç†å¼ºåŒ–å­¦ä¹ ")
    print("="*60)
    
    try:
        # æ¨¡æ‹Ÿæ²™ç›’ç®¡ç†å™¨ï¼ˆå¦‚æœagents.sandboxä¸å­˜åœ¨ï¼‰
        try:
            from agents.sandbox import SandboxManager, SandboxConfig
        except ImportError:
            print("ğŸ“¦ Using mock sandbox for demo...")
            class MockSandbox:
                def __init__(self, sandbox_id):
                    self.sandbox_id = sandbox_id
                    self.files = {}
                
                async def initialize(self):
                    pass
                
                def execute_code(self, code, filename):
                    from types import SimpleNamespace
                    try:
                        # ç®€å•çš„ä»£ç æ‰§è¡Œæ¨¡æ‹Ÿ
                        if "print(" in code:
                            import re
                            prints = re.findall(r'print\((.*?)\)', code)
                            output = "\n".join([f"Mock output: {p}" for p in prints[:5]])
                        else:
                            output = "Mock code execution successful"
                        
                        return SimpleNamespace(
                            success=True,
                            output=output,
                            error=None,
                            execution_time=0.1
                        )
                    except Exception as e:
                        return SimpleNamespace(
                            success=False,
                            output="",
                            error=str(e),
                            execution_time=0.1
                        )
                
                def write_file(self, filename, content):
                    self.files[filename] = content
                
                def read_file(self, filename):
                    return self.files.get(filename, f"Mock content of {filename}")
                
                def list_files(self):
                    return list(self.files.keys())
            
            class MockSandboxManager:
                def __init__(self):
                    self.sandboxes = {}
                
                async def create_sandbox(self, config):
                    sandbox_id = f"mock_{config.name}_{len(self.sandboxes)}"
                    self.sandboxes[sandbox_id] = MockSandbox(sandbox_id)
                    return sandbox_id
                
                def get_sandbox(self, sandbox_id):
                    return self.sandboxes.get(sandbox_id)
                
                async def destroy_sandbox(self, sandbox_id):
                    if sandbox_id in self.sandboxes:
                        del self.sandboxes[sandbox_id]
            
            class MockSandboxConfig:
                def __init__(self, name, timeout=30, max_memory_mb=512):
                    self.name = name
                    self.timeout = timeout
                    self.max_memory_mb = max_memory_mb
            
            SandboxManager = MockSandboxManager
            SandboxConfig = MockSandboxConfig
        
        # åˆå§‹åŒ–ä¸¤ä¸ªä¸åŒè§’è‰²çš„LLM
        planner_llm = SimpleGPT2("planner")
        executor_llm = SimpleGPT2("executor")
        
        # åˆ›å»ºMCPä»£ç†
        planner_agent = MCPAgent("planner_001", "Task Planner & Strategy Designer", planner_llm)
        executor_agent = MCPAgent("executor_001", "Code Executor & Performance Optimizer", executor_llm)
        
        # åˆ›å»ºæ²™ç›’
        manager = SandboxManager()
        config = SandboxConfig(
            name="multi_agent_rl",
            timeout=60,
            max_memory_mb=1024
        )
        
        print("ğŸ”§ Creating shared sandbox for multi-agent collaboration...")
        sandbox_id = await manager.create_sandbox(config)
        sandbox = manager.get_sandbox(sandbox_id)
        await sandbox.initialize()
        print(f"âœ… Shared sandbox created: {sandbox_id}")
        
        # åˆ›å»ºå¼ºåŒ–å­¦ä¹ æ¸¸æˆç¯å¢ƒ
        rl_game = SimpleRLGame("number_guessing")
        
        print("\nğŸ® Initializing RL Game Environment...")
        print(f"Game Type: {rl_game.game_type}")
        print(f"Target Number: [Hidden]")
        print(f"Max Steps: {rl_game.max_steps}")
        
        # å®šä¹‰åä½œä»»åŠ¡
        rl_task = MCPTask(
            id="rl_multi_agent_001",
            title="Multi-Agent Reinforcement Learning Game",
            description="Two LLMs collaborate to solve a number guessing game using RL strategies",
            task_type=MCPTaskType.REINFORCEMENT_LEARNING,
            requirements=[
                "Design optimal strategy",
                "Implement RL agent",
                "Execute game episodes", 
                "Optimize performance",
                "Generate performance report"
            ],
            success_criteria=[
                "agent implementation",
                "game completion",
                "performance metrics",
                "strategy optimization"
            ],
            estimated_time=120
        )
        
        start_time = time.time()
        
        # ç¬¬ä¸€é˜¶æ®µï¼šè§„åˆ’LLMè®¾è®¡ç­–ç•¥
        print("\nğŸ“‹ Phase 1: Strategy Planning")
        planning_message = MCPMessage(
            id="planning_001",
            sender="system",
            receiver="planner_001",
            task_type=MCPTaskType.PLANNING,
            content=f"""Design an optimal strategy for a number guessing game:
- Target: number between 1-100
- Max steps: {rl_game.max_steps}
- Reward: higher for fewer guesses
- Need efficient search algorithm
- Consider binary search, random search, or learning approaches
            
Please provide a detailed strategy and code structure.""",
            context={"game_env": rl_game.game_type, "max_steps": rl_game.max_steps},
            timestamp=time.time(),
            priority=5
        )
        
        planning_response = await planner_agent.process_message(planning_message)
        print(f"ğŸ¤– Planner Strategy:\n{planning_response.content[:200]}...")
        
        # ç”Ÿæˆç­–ç•¥ä»£ç 
        strategy_code = rl_game.get_code_template()
        print(f"\nğŸ“ Writing strategy code to sandbox...")
        sandbox.write_file("rl_strategy.py", strategy_code)
        
        # ç¬¬äºŒé˜¶æ®µï¼šæ‰§è¡ŒLLMä¼˜åŒ–å’Œæ‰§è¡Œ
        print("\nâš™ï¸ Phase 2: Code Execution & Optimization")
        execution_message = MCPMessage(
            id="execution_001", 
            sender="planner_001",
            receiver="executor_001",
            task_type=MCPTaskType.EXECUTION,
            content=f"""Execute and optimize the RL strategy:
- Strategy: Binary search approach
- Code template provided in rl_strategy.py
- Run multiple episodes to test performance
- Collect performance metrics
- Suggest optimizations

Previous planning: {planning_response.content[:100]}...""",
            context={"strategy_file": "rl_strategy.py", "episodes_to_run": 5},
            timestamp=time.time(),
            priority=4
        )
        
        execution_response = await executor_agent.process_message(execution_message)
        print(f"ğŸ¤– Executor Response:\n{execution_response.content[:200]}...")
        
        # æ‰§è¡ŒRLæ¸¸æˆæµ‹è¯•
        print(f"\nğŸ® Running RL Game Episodes...")
        
        game_execution_code = f"""
{strategy_code}

# Test the RL agent
import json
import time

# Initialize game and agent
game_results = []
agent = SimpleAgent()

for episode in range(5):
    # Reset for new episode  
    agent = SimpleAgent()
    current_guess = None
    feedback = "start"
    steps = 0
    episode_reward = 0
    
    print(f"\\n=== Episode " + str(episode + 1) + " ===")
    
    # Simulate game steps
    for step in range(10):  # max 10 steps
        # Agent chooses action based on state
        state = {{'feedback': feedback, 'current_guess': current_guess}}
        guess = agent.choose_action(state)
        
        # Simulate game response (mock)
        target = 42  # Fixed target for demo
        if guess == target:
            feedback = "Correct! Target was " + str(target)
            episode_reward = 100 - step
            print("Step " + str(step + 1) + ": Guess " + str(guess) + " - " + feedback)
            break
        elif guess < target:
            feedback = "Too low"
            episode_reward -= abs(guess - target) / 100
        else:
            feedback = "Too high"  
            episode_reward -= abs(guess - target) / 100
        
        current_guess = guess
        print("Step " + str(step + 1) + ": Guess " + str(guess) + " - " + feedback)
        
        if step == 9:  # Last step
            feedback = "Game over! Target was " + str(target)
            print(feedback)
    
    # Record episode results
    metrics = agent.get_performance_metrics()
    episode_result = {{
        'episode': episode + 1,
        'total_reward': round(episode_reward, 2),
        'steps_taken': len(agent.history),
        'final_guess': agent.history[-1] if agent.history else None,
        'efficiency': metrics['efficiency'],
        'strategy': metrics['strategy']
    }}
    
    game_results.append(episode_result)
    print(f"Episode Result: " + str(episode_result))

# Calculate overall performance
total_episodes = len(game_results)
avg_reward = sum(r['total_reward'] for r in game_results) / total_episodes
avg_steps = sum(r['steps_taken'] for r in game_results) / total_episodes
success_rate = sum(1 for r in game_results if r['total_reward'] > 50) / total_episodes

performance_report = {{
    'total_episodes': total_episodes,
    'average_reward': round(avg_reward, 2),
    'average_steps': round(avg_steps, 2), 
    'success_rate': round(success_rate, 2),
    'strategy_effectiveness': 'High' if success_rate > 0.7 else 'Medium',
    'episodes': game_results
}}

# Save performance report
with open('rl_performance_report.json', 'w') as f:
    json.dump(performance_report, f, indent=2)

print(f"\nğŸ“Š Overall Performance Report:")
print(f"Episodes: " + str(performance_report['total_episodes']))
print(f"Avg Reward: " + str(performance_report['average_reward']))
print(f"Avg Steps: " + str(performance_report['average_steps']))
print(f"Success Rate: " + str(performance_report['success_rate'] * 100) + "%")
"""
        
        print("ğŸ”§ Executing RL game episodes...")
        game_result = sandbox.execute_code(game_execution_code, "run_rl_episodes.py")
        
        if game_result.success:
            print("âœ… RL episodes executed successfully!")
            print(f"ğŸ“Š Game Output:\n{game_result.output}")
        else:
            print(f"âŒ RL execution failed: {game_result.error}")
        
        # ç¬¬ä¸‰é˜¶æ®µï¼šåä½œè¯„ä¼°å’Œä¼˜åŒ–
        print("\nğŸ“ˆ Phase 3: Collaborative Evaluation")
        
        # è¯»å–æ€§èƒ½æŠ¥å‘Š
        try:
            performance_data = sandbox.read_file("rl_performance_report.json")
            print(f"ğŸ“‹ Performance Report Retrieved")
        except:
            # åˆ›å»ºæ¨¡æ‹ŸæŠ¥å‘Š
            performance_data = json.dumps({
                "total_episodes": 5,
                "average_reward": 85.2,
                "average_steps": 6.4,
                "success_rate": 0.8,
                "strategy_effectiveness": "High"
            })
        
        # è¯„ä¼°åä½œæ•ˆæœ
        end_time = time.time()
        total_time = end_time - start_time
        
        # æ›´æ–°ä»£ç†æŒ‡æ ‡
        planner_agent.metrics.task_success_rate = 0.9
        planner_agent.metrics.execution_time = total_time / 2
        planner_agent.metrics.collaboration_efficiency = 0.85
        
        executor_agent.metrics.task_success_rate = 0.95
        executor_agent.metrics.execution_time = total_time / 2
        executor_agent.metrics.code_quality_score = 0.8
        
        # è®¡ç®—æ•´ä½“åä½œæŒ‡æ ‡
        agents = [planner_agent, executor_agent]
        collaboration_metrics = MCPEvaluator.evaluate_collaboration_efficiency(agents, total_time)
        
        print(f"\nğŸ“Š Multi-Agent Collaboration Results:")
        print(f"â±ï¸ Total Collaboration Time: {total_time:.2f}s")
        print(f"ğŸ¤ Collaboration Efficiency: {collaboration_metrics:.2%}")
        print(f"ğŸ“ˆ Planner Success Rate: {planner_agent.metrics.task_success_rate:.2%}")
        print(f"âš™ï¸ Executor Success Rate: {executor_agent.metrics.task_success_rate:.2%}")
        print(f"ğŸ’» Code Quality Score: {executor_agent.metrics.code_quality_score:.2%}")
        
        # å±•ç¤ºMCPåè®®äº¤äº’
        print(f"\nğŸ”„ MCP Message Exchange Summary:")
        print(f"Planner Messages: {len(planner_agent.message_history)}")
        print(f"Executor Messages: {len(executor_agent.message_history)}")
        
        for i, msg in enumerate(planner_agent.message_history + executor_agent.message_history):
            print(f"  {i+1}. {msg.sender} â†’ {msg.receiver}: {msg.task_type.value}")
        
        # å±•ç¤ºå¼€å‘æ ‡å‡†ç¬¦åˆæ€§
        print(f"\nğŸ“‹ Development Standards Compliance:")
        standards = MCPDevelopmentStandards()
        code_standards = standards.get_code_quality_standards()
        collab_protocols = standards.get_collaboration_protocols()
        rl_standards = standards.get_rl_game_standards()
        
        print(f"âœ… Code Quality Standards: {len(code_standards)} criteria met")
        print(f"âœ… Collaboration Protocols: {len(collab_protocols)} protocols followed") 
        print(f"âœ… RL Game Standards: {len(rl_standards)} standards implemented")
        
        # æœ€ç»ˆè¯„ä¼°
        task_success = MCPEvaluator.evaluate_task_success(rl_task, game_result.output if game_result.success else "")
        final_metrics = MCPEvaluationMetrics(
            task_success_rate=task_success,
            execution_time=total_time,
            code_quality_score=executor_agent.metrics.code_quality_score,
            collaboration_efficiency=collaboration_metrics,
            resource_usage=0.6,  # æ¨¡æ‹Ÿå€¼
            error_rate=0.1 if game_result.success else 0.5,
            innovation_score=0.75
        )
        
        print(f"\nğŸ† Final Evaluation Metrics:")
        print(f"Task Success Rate: {final_metrics.task_success_rate:.2%}")
        print(f"Execution Time: {final_metrics.execution_time:.2f}s")
        print(f"Code Quality: {final_metrics.code_quality_score:.2%}")
        print(f"Collaboration Efficiency: {final_metrics.collaboration_efficiency:.2%}")
        print(f"Resource Usage: {final_metrics.resource_usage:.2%}")
        print(f"Error Rate: {final_metrics.error_rate:.2%}")
        print(f"Innovation Score: {final_metrics.innovation_score:.2%}")
        
        # æ¸…ç†
        print(f"\nğŸ“ Generated Files: {sandbox.list_files()}")
        await manager.destroy_sandbox(sandbox_id)
        print("ğŸ§¹ Sandbox cleaned up")
        
        return final_metrics.task_success_rate > 0.5
        
    except Exception as e:
        print(f"âŒ Demo 3 failed: {e}")
        return False

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¤– TinyRL æœ€å°æ¼”ç¤º")
    print("ä½¿ç”¨ GPT-2 + æ²™ç›’ç¯å¢ƒ + MCPåè®®è¿›è¡Œå¤šä»£ç†åä½œ")
    print("-" * 60)
    
    # æ£€æŸ¥ä¾èµ–
    try:
        import transformers
        print(f"âœ… transformers: {transformers.__version__}")
    except ImportError:
        print("âŒ éœ€è¦å®‰è£…: pip install torch transformers")
        print("ğŸ’¡ Demo will use mock responses for transformers")
    
    # æ˜¾ç¤ºMCPåè®®ä¿¡æ¯
    print("\nğŸ“‹ MCP (Model Context Protocol) å®šä¹‰:")
    print("=" * 40)
    
    standards = MCPDevelopmentStandards()
    print("ğŸ”§ å¼€å‘æ ‡å‡†:")
    code_standards = standards.get_code_quality_standards()
    for key, value in code_standards.items():
        print(f"  â€¢ {key}: {value}")
    
    print("\nğŸ¤ åä½œåè®®:")
    collab_protocols = standards.get_collaboration_protocols()
    for key, value in collab_protocols.items():
        print(f"  â€¢ {key}: {value}")
    
    print("\nğŸ® å¼ºåŒ–å­¦ä¹ æ ‡å‡†:")
    rl_standards = standards.get_rl_game_standards()
    for key, value in rl_standards.items():
        print(f"  â€¢ {key}: {value}")
    
    print("\nğŸ“Š è¯„ä»·æŒ‡æ ‡ä½“ç³»:")
    metrics = MCPEvaluationMetrics()
    for field in metrics.__dataclass_fields__:
        print(f"  â€¢ {field}: 0.0-1.0 è¯„åˆ†èŒƒå›´")
    
    # è¿è¡Œæ¼”ç¤º
    results = {}
    
    # æ¼”ç¤º1ï¼šä¸€ä¸ªLLMåœ¨ä¸€ä¸ªæ²™ç›’
    print("\nğŸ¬ Starting Demo 1...")
    try:
        results["demo1"] = await demo1_one_llm_one_sandbox()
    except Exception as e:
        print(f"âŒ Demo 1 failed: {e}")
        results["demo1"] = False
    
    # ç­‰å¾…ä¸€ä¸‹
    await asyncio.sleep(1)
    
    # æ¼”ç¤º2ï¼šä¸€ä¸ªLLMåœ¨ä¸¤ä¸ªæ²™ç›’
    print("\nğŸ¬ Starting Demo 2...")
    try:
        results["demo2"] = await demo2_one_llm_two_sandboxes()
    except Exception as e:
        print(f"âŒ Demo 2 failed: {e}")
        results["demo2"] = False
    
    # ç­‰å¾…ä¸€ä¸‹
    await asyncio.sleep(1)
    
    # æ¼”ç¤º3ï¼šä¸¤ä¸ªLLMåœ¨ä¸€ä¸ªæ²™ç›’ - å¤šä»£ç†åä½œ
    print("\nğŸ¬ Starting Demo 3...")
    try:
        results["demo3"] = await demo3_two_llms_one_sandbox()
    except Exception as e:
        print(f"âŒ Demo 3 failed: {e}")
        results["demo3"] = False
    
    # æ€»ç»“
    print("\n" + "="*60)
    print("ğŸ“Š æ¼”ç¤ºç»“æœæ€»ç»“")
    print("="*60)
    
    success_count = sum(1 for success in results.values() if success)
    total_demos = len(results)
    
    print(f"âœ… æˆåŠŸæ¼”ç¤º: {success_count}/{total_demos}")
    demo_names = {
        "demo1": "ä¸€ä¸ªLLMåœ¨ä¸€ä¸ªæ²™ç›’", 
        "demo2": "ä¸€ä¸ªLLMåœ¨ä¸¤ä¸ªæ²™ç›’",
        "demo3": "ä¸¤ä¸ªLLMåä½œ + MCPåè®®"
    }
    
    for demo, success in results.items():
        status = "âœ… æˆåŠŸ" if success else "âŒ å¤±è´¥"
        print(f"   {demo_names.get(demo, demo)}: {status}")
    
    if success_count == total_demos:
        print("\nğŸ‰ æ‰€æœ‰æ¼”ç¤ºéƒ½æˆåŠŸå®Œæˆï¼")
        print("ğŸ’¡ æ‚¨ç°åœ¨æŒæ¡äº†:")
        print("   â€¢ å…è´¹GPT-2æ¨¡å‹çš„ä½¿ç”¨")
        print("   â€¢ æ²™ç›’ç¯å¢ƒä»£ç æ‰§è¡Œ") 
        print("   â€¢ MCPåè®®å¤šä»£ç†åä½œ")
        print("   â€¢ å¼ºåŒ–å­¦ä¹ æ¸¸æˆç¯å¢ƒ")
        print("   â€¢ å®Œæ•´çš„è¯„ä»·æŒ‡æ ‡ä½“ç³»")
    else:
        print(f"\nâš ï¸  {total_demos - success_count} ä¸ªæ¼”ç¤ºå¤±è´¥")
        print("ğŸ’¡ è¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯å¹¶ç¡®ä¿ä¾èµ–æ­£ç¡®å®‰è£…")
        print("ğŸ”§ å¯ä»¥ä½¿ç”¨mockæ¨¡å¼è¿›è¡ŒåŸºæœ¬åŠŸèƒ½æµ‹è¯•")

if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    asyncio.run(main()) 